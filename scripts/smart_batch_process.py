#!/usr/bin/env python3
"""
æ™ºèƒ½æ‰¹é‡è™•ç†ä½å®¿è³‡æ–™ - ä½¿ç”¨å¿«å–å’Œå»é‡ç­–ç•¥
"""

import sys
import os
import json
import uuid
import time
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'src'))

from itinerary_planner.infrastructure.services.geocoding_service import geocoding_service
from itinerary_planner.infrastructure.persistence.database import SessionLocal
from itinerary_planner.infrastructure.persistence.orm_models import Accommodation
from geoalchemy2 import WKTElement

def load_existing_accommodations():
    """è¼‰å…¥ç¾æœ‰ä½å®¿è³‡æ–™ï¼Œé¿å…é‡è¤‡"""
    
    db = SessionLocal()
    try:
        existing = db.query(Accommodation.name, Accommodation.address).all()
        existing_set = set()
        for name, addr in existing:
            existing_set.add((name.lower().strip(), addr.lower().strip()))
        return existing_set
    finally:
        db.close()

def clean_and_deduplicate_addresses(data_list, data_type):
    """æ¸…ç†ä¸¦å»é‡åœ°å€"""
    
    unique_addresses = {}
    
    for item in data_list:
        name = item.get("ä¸­æ–‡åç¨±", "").strip()
        address = item.get("ç‡Ÿæ¥­åœ°å€", "").strip()
        phone = item.get("é›»è©±", "").strip()
        
        if not name or not address:
            continue
            
        # æ¸…ç†åœ°å€ - ç§»é™¤é–€ç‰Œè™Ÿç¢¼
        import re
        clean_address = re.sub(r'\d+[-\d]*è™Ÿ.*$', '', address)
        clean_address = re.sub(r'\d+[-\d]*æ¨“.*$', '', clean_address)
        clean_address = clean_address.strip()
        
        # ç¢ºä¿åŒ…å«å®œè˜­
        if "å®œè˜­" not in clean_address:
            clean_address = f"å®œè˜­ç¸£{clean_address}"
        
        # ä½¿ç”¨æ¸…ç†å¾Œçš„åœ°å€ä½œç‚ºkey
        if clean_address not in unique_addresses:
            unique_addresses[clean_address] = {
                "name": name,
                "phone": phone,
                "type": data_type,
                "original_address": address
            }
    
    return unique_addresses

def batch_geocode_with_cache(addresses, cache_file="geocoding_cache.json"):
    """å¸¶å¿«å–çš„æ‰¹é‡åœ°å€è§£æ"""
    
    # è¼‰å…¥å¿«å–
    cache = {}
    if os.path.exists(cache_file):
        try:
            with open(cache_file, 'r', encoding='utf-8') as f:
                cache = json.load(f)
        except:
            cache = {}
    
    # æª¢æŸ¥å“ªäº›åœ°å€éœ€è¦è§£æ
    to_geocode = []
    results = {}
    
    for addr in addresses:
        if addr in cache:
            results[addr] = cache[addr]
        else:
            to_geocode.append(addr)
    
    print(f"ğŸ“‹ å¿«å–å‘½ä¸­: {len(results)} ç­†")
    print(f"ğŸ”„ éœ€è¦è§£æ: {len(to_geocode)} ç­†")
    
    if to_geocode:
        # åˆ†æ‰¹è™•ç†ï¼Œé¿å…APIé™åˆ¶
        batch_size = 20
        for i in range(0, len(to_geocode), batch_size):
            batch = to_geocode[i:i+batch_size]
            print(f"  ğŸ”„ è™•ç†æ‰¹æ¬¡ {i//batch_size + 1}: {len(batch)} ç­†")
            
            batch_results = geocoding_service.batch_geocode(batch)
            
            # æ›´æ–°å¿«å–å’Œçµæœ
            for addr, coords in batch_results.items():
                cache[addr] = coords
                results[addr] = coords
            
            # å„²å­˜å¿«å–
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(cache, f, ensure_ascii=False, indent=2)
            
            # é¿å…APIé™åˆ¶
            if i + batch_size < len(to_geocode):
                time.sleep(2)
    
    return results

def import_accommodations(accommodation_data, geocoded_results):
    """åŒ¯å…¥ä½å®¿è³‡æ–™åˆ°è³‡æ–™åº«"""
    
    db = SessionLocal()
    try:
        imported_count = 0
        skipped_count = 0
        
        for address, (lat, lon) in geocoded_results.items():
            if address not in accommodation_data:
                continue
                
            info = accommodation_data[address]
            
            # æª¢æŸ¥åº§æ¨™æ˜¯å¦åœ¨å®œè˜­ç¯„åœ
            if not (24.4 <= lat <= 25.0 and 121.4 <= lon <= 122.0):
                skipped_count += 1
                continue
            
            # æª¢æŸ¥æ˜¯å¦å·²å­˜åœ¨
            existing = db.query(Accommodation).filter(
                Accommodation.name.ilike(f"%{info['name']}%")
            ).first()
            
            if existing:
                skipped_count += 1
                continue
            
            # å‰µå»ºæ–°è¨˜éŒ„
            accommodation = Accommodation(
                id=uuid.uuid4(),
                name=info["name"],
                geom=WKTElement(f"POINT({lon} {lat})", srid=4326),
                type=info["type"],
                price_range=3,  # é è¨­ä¸­æª”
                rating=3.5,  # é è¨­è©•åˆ†
                address=info["original_address"],
                phone=info["phone"],
                amenities=["WiFi", "åœè»Šå ´"]  # é è¨­è¨­æ–½
            )
            
            db.add(accommodation)
            imported_count += 1
        
        db.commit()
        
        print(f"âœ… æˆåŠŸåŒ¯å…¥: {imported_count} ç­†")
        print(f"â­ï¸ è·³éé‡è¤‡: {skipped_count} ç­†")
        
        return imported_count
        
    except Exception as e:
        print(f"âŒ åŒ¯å…¥éŒ¯èª¤: {e}")
        db.rollback()
        return 0
    finally:
        db.close()

def process_hotels():
    """è™•ç†æ—…é¤¨è³‡æ–™"""
    
    print("ğŸ¨ **è™•ç†å®œè˜­ç¸£æ—…é¤¨åå†Š**")
    print("=" * 50)
    
    hotel_file = "/home/johnny/å°ˆæ¡ˆ/æ¯”è³½è³‡æ–™/docs/å®œè˜­ç¸£æ—…é¤¨åå†Š.json"
    
    with open(hotel_file, 'r', encoding='utf-8') as f:
        hotels = json.load(f)
    
    print(f"ğŸ“Š åŸå§‹è³‡æ–™: {len(hotels)} ç­†")
    
    # æ¸…ç†ä¸¦å»é‡
    hotel_data = clean_and_deduplicate_addresses(hotels, "hotel")
    print(f"ğŸ“ å»é‡å¾Œ: {len(hotel_data)} ç­†")
    
    # åœ°å€è§£æ
    addresses = list(hotel_data.keys())
    geocoded_results = batch_geocode_with_cache(addresses)
    
    # åŒ¯å…¥è³‡æ–™åº«
    imported = import_accommodations(hotel_data, geocoded_results)
    
    return imported

def process_homestays():
    """è™•ç†æ°‘å®¿è³‡æ–™"""
    
    print("\nğŸ¡ **è™•ç†å®œè˜­ç¸£æ°‘å®¿åå†Š**")
    print("=" * 50)
    
    homestay_file = "/home/johnny/å°ˆæ¡ˆ/æ¯”è³½è³‡æ–™/docs/å®œè˜­ç¸£æ°‘å®¿åå†Š.json"
    
    with open(homestay_file, 'r', encoding='utf-8') as f:
        homestays = json.load(f)
    
    print(f"ğŸ“Š åŸå§‹è³‡æ–™: {len(homestays)} ç­†")
    
    # åªè™•ç†å‰100ç­†é¿å…APIé™åˆ¶
    homestays = homestays[:100]
    print(f"ğŸ”„ å¯¦éš›è™•ç†: {len(homestays)} ç­†")
    
    # æ¸…ç†ä¸¦å»é‡
    homestay_data = clean_and_deduplicate_addresses(homestays, "homestay")
    print(f"ğŸ“ å»é‡å¾Œ: {len(homestay_data)} ç­†")
    
    # åœ°å€è§£æ
    addresses = list(homestay_data.keys())
    geocoded_results = batch_geocode_with_cache(addresses)
    
    # åŒ¯å…¥è³‡æ–™åº«
    imported = import_accommodations(homestay_data, geocoded_results)
    
    return imported

def main():
    """ä¸»åŸ·è¡Œå‡½æ•¸"""
    
    print("ğŸš€ **æ™ºèƒ½æ‰¹é‡è™•ç†ä½å®¿è³‡æ–™**")
    print("=" * 60)
    
    # è¼‰å…¥ç¾æœ‰è³‡æ–™
    existing = load_existing_accommodations()
    print(f"ğŸ“‹ ç¾æœ‰ä½å®¿: {len(existing)} ç­†")
    
    # è™•ç†æ—…é¤¨
    hotel_count = process_hotels()
    
    # è™•ç†æ°‘å®¿
    homestay_count = process_homestays()
    
    print(f"\nğŸ‰ **è™•ç†å®Œæˆï¼**")
    print(f"  ğŸ¨ æ–°å¢æ—…é¤¨: {hotel_count} ç­†")
    print(f"  ğŸ¡ æ–°å¢æ°‘å®¿: {homestay_count} ç­†")
    print(f"  ğŸ“Š ç¸½è¨ˆæ–°å¢: {hotel_count + homestay_count} ç­†")

if __name__ == "__main__":
    main()
