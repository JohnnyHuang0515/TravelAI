#!/usr/bin/env python3
"""
çµ±ä¸€çš„è³‡æ–™åŒ¯å…¥ç®¡é“
ä½¿ç”¨æ–°çš„è³‡æ–™è™•ç†ç®¡é“ï¼Œåœ¨åŒ¯å…¥æ™‚å°±å®Œæˆæ‰€æœ‰è™•ç†
"""

import sys
import os
import json
import logging
from typing import Dict, Any, List
from pathlib import Path

# æ·»åŠ å°ˆæ¡ˆè·¯å¾‘
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'src'))
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from itinerary_planner.infrastructure.persistence.database import SessionLocal
from itinerary_planner.infrastructure.persistence.orm_models import Place, Accommodation
from itinerary_planner.infrastructure.data_processing.data_pipeline import DataProcessingPipeline, ProcessedData
from geoalchemy2 import WKTElement
from scripts.deduplication_manager import DeduplicationManager

# è¨­å®šæ—¥èªŒ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class UnifiedDataImporter:
    """çµ±ä¸€çš„è³‡æ–™åŒ¯å…¥å™¨"""
    
    def __init__(self):
        self.pipeline = DataProcessingPipeline()
        self.db = SessionLocal()
        self.deduplicator = DeduplicationManager()
    
    def _get_source_type(self, file_path: str) -> str:
        """æ ¹æ“šæª”æ¡ˆåç¨±åˆ¤æ–·è³‡æ–™ä¾†æºé¡å‹"""
        filename = Path(file_path).name
        
        if "ç’°ä¿æ¨™ç« æ—…é¤¨" in filename:
            return "eco_hotel"
        elif "ç’°ä¿é¤å»³" in filename:
            return "eco_restaurant"
        elif "ç’°å¢ƒæ•™è‚²è¨­æ–½" in filename:
            return "education_facility"
        elif "æ—…é¤¨åå†Š" in filename:
            return "yilan_hotel"
        elif "æ°‘å®¿åå†Š" in filename:
            return "yilan_bnb"
        else:
            return "default"
    
    def __enter__(self):
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        self.db.close()
    
    def clear_existing_data(self):
        """æ¸…é™¤ç¾æœ‰è³‡æ–™"""
        logger.info("æ¸…é™¤ç¾æœ‰è³‡æ–™...")
        
        try:
            # æ¸…é™¤åœ°é»
            self.db.query(Place).delete()
            logger.info("å·²æ¸…é™¤åœ°é»è³‡æ–™")
            
            # æ¸…é™¤ä½å®¿
            self.db.query(Accommodation).delete()
            logger.info("å·²æ¸…é™¤ä½å®¿è³‡æ–™")
            
            self.db.commit()
            logger.info("è³‡æ–™æ¸…é™¤å®Œæˆ")
            
        except Exception as e:
            logger.error(f"æ¸…é™¤è³‡æ–™å¤±æ•—: {e}")
            self.db.rollback()
            raise
    
    def load_json_data(self, file_path: str) -> List[Dict[str, Any]]:
        """è¼‰å…¥ JSON è³‡æ–™"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            logger.info(f"æˆåŠŸè¼‰å…¥ {file_path}: {len(data)} ç­†è³‡æ–™")
            return data
            
        except Exception as e:
            logger.error(f"è¼‰å…¥ {file_path} å¤±æ•—: {e}")
            raise
    
    def convert_processed_data_to_place(self, processed: ProcessedData) -> Place:
        """å°‡è™•ç†å¾Œçš„è³‡æ–™è½‰æ›ç‚º Place ç‰©ä»¶"""
        place = Place(
            name=processed.name,
            description=processed.description,
            address=processed.address,
            categories=processed.categories,
            rating=processed.rating,
            price_range=processed.price_range,
            stay_minutes=processed.stay_minutes,
            place_metadata=processed.context_metadata
            # embedding=processed.embedding  # æš«æ™‚ç§»é™¤
        )
        
        # è¨­å®šåœ°ç†ä½ç½®
        if processed.latitude and processed.longitude:
            place.geom = WKTElement(f"POINT({processed.longitude} {processed.latitude})", srid=4326)
        
        return place
    
    def convert_processed_data_to_accommodation(self, processed: ProcessedData) -> Accommodation:
        """å°‡è™•ç†å¾Œçš„è³‡æ–™è½‰æ›ç‚º Accommodation ç‰©ä»¶"""
        accommodation = Accommodation(
            name=processed.name,
            address=processed.address,
            type=processed.type,
            rating=processed.rating,
            price_range=processed.price_range,
            amenities=processed.amenities
            # embedding=processed.embedding  # æš«æ™‚ç§»é™¤
        )
        
        # è¨­å®šåœ°ç†ä½ç½®
        if processed.latitude and processed.longitude:
            accommodation.geom = WKTElement(f"POINT({processed.longitude} {processed.latitude})", srid=4326)
        
        return accommodation
    
    def import_places_from_file(self, file_path: str) -> int:
        """å¾æª”æ¡ˆåŒ¯å…¥åœ°é»è³‡æ–™"""
        logger.info(f"é–‹å§‹åŒ¯å…¥åœ°é»è³‡æ–™: {file_path}")
        
        try:
            # è¼‰å…¥åŸå§‹è³‡æ–™
            raw_data_list = self.load_json_data(file_path)
            
            # å»é‡è™•ç†
            source_type = self._get_source_type(file_path)
            if source_type in ["eco_hotel", "eco_restaurant", "education_facility"]:
                # å°æ–°è³‡æ–™é€²è¡Œå»é‡
                duplicate_groups = self.deduplicator.find_duplicates(raw_data_list, "place")
                if duplicate_groups:
                    logger.info(f"ç™¼ç¾ {len(duplicate_groups)} å€‹é‡è¤‡çµ„")
                    # é¸æ“‡æœ€ä½³ç‰ˆæœ¬
                    resolved_items = self.deduplicator.resolve_duplicates(duplicate_groups, "eco_certified")
                    # ç§»é™¤é‡è¤‡é …ç›®
                    for group in duplicate_groups.values():
                        for item in group[1:]:  # ä¿ç•™ç¬¬ä¸€å€‹ï¼Œç§»é™¤å…¶ä»–
                            if item in raw_data_list:
                                raw_data_list.remove(item)
            
            # æ‰¹æ¬¡è™•ç†è³‡æ–™
            processed_list = self.pipeline.batch_process(raw_data_list, "place", source_type)
            
            # è½‰æ›ç‚º ORM ç‰©ä»¶ä¸¦å„²å­˜
            imported_count = 0
            for i, processed in enumerate(processed_list):
                try:
                    place = self.convert_processed_data_to_place(processed)
                    self.db.add(place)
                    imported_count += 1
                    
                    if (i + 1) % 50 == 0:
                        logger.info(f"å·²è™•ç† {i + 1}/{len(processed_list)} å€‹åœ°é»")
                        
                except Exception as e:
                    logger.error(f"è™•ç†åœ°é» {processed.name} å¤±æ•—: {e}")
                    continue
            
            self.db.commit()
            logger.info(f"åœ°é»åŒ¯å…¥å®Œæˆ: {imported_count}/{len(raw_data_list)} ç­†æˆåŠŸ")
            return imported_count
            
        except Exception as e:
            logger.error(f"åŒ¯å…¥åœ°é»å¤±æ•—: {e}")
            self.db.rollback()
            raise
    
    def import_accommodations_from_file(self, file_path: str) -> int:
        """å¾æª”æ¡ˆåŒ¯å…¥ä½å®¿è³‡æ–™"""
        logger.info(f"é–‹å§‹åŒ¯å…¥ä½å®¿è³‡æ–™: {file_path}")
        
        try:
            # è¼‰å…¥åŸå§‹è³‡æ–™
            raw_data_list = self.load_json_data(file_path)
            
            # å»é‡è™•ç†
            source_type = self._get_source_type(file_path)
            if source_type == "eco_hotel":
                # å°ç’°ä¿æ¨™ç« æ—…é¤¨é€²è¡Œå»é‡
                duplicate_groups = self.deduplicator.find_duplicates(raw_data_list, "accommodation")
                if duplicate_groups:
                    logger.info(f"ç™¼ç¾ {len(duplicate_groups)} å€‹é‡è¤‡çµ„")
                    # é¸æ“‡æœ€ä½³ç‰ˆæœ¬
                    resolved_items = self.deduplicator.resolve_duplicates(duplicate_groups, "eco_certified")
                    # ç§»é™¤é‡è¤‡é …ç›®
                    for group in duplicate_groups.values():
                        for item in group[1:]:  # ä¿ç•™ç¬¬ä¸€å€‹ï¼Œç§»é™¤å…¶ä»–
                            if item in raw_data_list:
                                raw_data_list.remove(item)
            
            # æ‰¹æ¬¡è™•ç†è³‡æ–™
            processed_list = self.pipeline.batch_process(raw_data_list, "accommodation", source_type)
            
            # è½‰æ›ç‚º ORM ç‰©ä»¶ä¸¦å„²å­˜
            imported_count = 0
            for i, processed in enumerate(processed_list):
                try:
                    accommodation = self.convert_processed_data_to_accommodation(processed)
                    self.db.add(accommodation)
                    imported_count += 1
                    
                    if (i + 1) % 100 == 0:
                        logger.info(f"å·²è™•ç† {i + 1}/{len(processed_list)} å€‹ä½å®¿")
                        
                except Exception as e:
                    logger.error(f"è™•ç†ä½å®¿ {processed.name} å¤±æ•—: {e}")
                    continue
            
            self.db.commit()
            logger.info(f"ä½å®¿åŒ¯å…¥å®Œæˆ: {imported_count}/{len(raw_data_list)} ç­†æˆåŠŸ")
            return imported_count
            
        except Exception as e:
            logger.error(f"åŒ¯å…¥ä½å®¿å¤±æ•—: {e}")
            self.db.rollback()
            raise
    
    def import_from_directory(self, data_dir: str, clear_existing: bool = True):
        """å¾ç›®éŒ„åŒ¯å…¥æ‰€æœ‰è³‡æ–™"""
        logger.info(f"é–‹å§‹å¾ç›®éŒ„åŒ¯å…¥è³‡æ–™: {data_dir}")
        
        if clear_existing:
            self.clear_existing_data()
        
        data_path = Path(data_dir)
        
        # çµ±è¨ˆè³‡è¨Š
        total_places = 0
        total_accommodations = 0
        
        try:
            # åŒ¯å…¥åœ°é»è³‡æ–™
            place_files = [
                "å®œè˜­ç¸£ç’°ä¿é¤å»³.json",
                "å®œè˜­ç¸£éŠæ†©é¡æ™¯é».json",
                "å®œè˜­ç¸£è‡ªç„¶é¢¨æ™¯é¡æ™¯é».json",
                "å®œè˜­ç¸£æ–‡åŒ–é¡æ™¯é».json",
                "å®œè˜­ç¸£æº«æ³‰é¡æ™¯é».json",
                "å®œè˜­ç¸£ç”œé»å†°å“.json",
                # å…¨å°ç£è³‡æ–™
                "ç’°ä¿é¤å»³ç’°å¢ƒå³æ™‚é€šåœ°åœ–è³‡æ–™.json",
                "ç’°å¢ƒæ•™è‚²è¨­æ–½å ´æ‰€èªè­‰è³‡æ–™.json"
            ]
            
            for file_name in place_files:
                file_path = data_path / file_name
                if file_path.exists():
                    count = self.import_places_from_file(str(file_path))
                    total_places += count
                else:
                    logger.warning(f"æª”æ¡ˆä¸å­˜åœ¨: {file_path}")
            
            # åŒ¯å…¥ä½å®¿è³‡æ–™
            accommodation_files = [
                "å®œè˜­ç¸£æ—…é¤¨åå†Š.json",
                "å®œè˜­ç¸£æ°‘å®¿åå†Š.json",
                # å…¨å°ç£è³‡æ–™
                "ç’°ä¿æ¨™ç« æ—…é¤¨ç’°å¢ƒå³æ™‚é€šåœ°åœ–è³‡æ–™.json"
            ]
            
            for file_name in accommodation_files:
                file_path = data_path / file_name
                if file_path.exists():
                    count = self.import_accommodations_from_file(str(file_path))
                    total_accommodations += count
                else:
                    logger.warning(f"æª”æ¡ˆä¸å­˜åœ¨: {file_path}")
            
            logger.info(f"è³‡æ–™åŒ¯å…¥å®Œæˆ!")
            logger.info(f"ç¸½è¨ˆåŒ¯å…¥:")
            logger.info(f"  ğŸ“ åœ°é»: {total_places} ç­†")
            logger.info(f"  ğŸ¨ ä½å®¿: {total_accommodations} ç­†")
            logger.info(f"  ğŸ“Š ç¸½è¨ˆ: {total_places + total_accommodations} ç­†")
            
        except Exception as e:
            logger.error(f"è³‡æ–™åŒ¯å…¥å¤±æ•—: {e}")
            raise
    
    def get_import_statistics(self):
        """å–å¾—åŒ¯å…¥çµ±è¨ˆè³‡è¨Š"""
        try:
            places_count = self.db.query(Place.id).count()
            accommodations_count = self.db.query(Accommodation.id).count()
            
            # æš«æ™‚ç§»é™¤ embedding çµ±è¨ˆ
            places_with_embedding = 0  # self.db.query(Place.id).filter(Place.embedding.isnot(None)).count()
            accommodations_with_embedding = 0  # self.db.query(Accommodation.id).filter(Accommodation.embedding.isnot(None)).count()
            
            places_with_metadata = self.db.query(Place.id).filter(Place.place_metadata.isnot(None)).count()
            
            places_with_coordinates = self.db.query(Place.id).filter(Place.geom.isnot(None)).count()
            accommodations_with_coordinates = self.db.query(Accommodation.id).filter(Accommodation.geom.isnot(None)).count()
            
            logger.info("ğŸ“Š **è³‡æ–™åº«çµ±è¨ˆè³‡è¨Š**")
            logger.info(f"ğŸ“ åœ°é»ç¸½æ•¸: {places_count}")
            logger.info(f"  - æœ‰ embedding: {places_with_embedding}")
            logger.info(f"  - æœ‰ metadata: {places_with_metadata}")
            logger.info(f"  - æœ‰åº§æ¨™: {places_with_coordinates}")
            
            logger.info(f"ğŸ¨ ä½å®¿ç¸½æ•¸: {accommodations_count}")
            logger.info(f"  - æœ‰ embedding: {accommodations_with_embedding}")
            logger.info(f"  - æœ‰åº§æ¨™: {accommodations_with_coordinates}")
            
        except Exception as e:
            logger.error(f"å–å¾—çµ±è¨ˆè³‡è¨Šå¤±æ•—: {e}")

def main():
    """ä¸»å‡½æ•¸"""
    import argparse
    
    parser = argparse.ArgumentParser(description='çµ±ä¸€è³‡æ–™åŒ¯å…¥å™¨')
    parser.add_argument('--data-dir', default='docs', help='è³‡æ–™ç›®éŒ„è·¯å¾‘')
    parser.add_argument('--clear', action='store_true', help='æ¸…é™¤ç¾æœ‰è³‡æ–™')
    parser.add_argument('--stats-only', action='store_true', help='åªé¡¯ç¤ºçµ±è¨ˆè³‡è¨Š')
    
    args = parser.parse_args()
    
    with UnifiedDataImporter() as importer:
        if args.stats_only:
            importer.get_import_statistics()
        else:
            importer.import_from_directory(args.data_dir, args.clear)
            importer.get_import_statistics()

if __name__ == "__main__":
    main()
